
package application.workflow;

import infra.DatalakeReader;
import service.DataMartService;

import java.time.LocalDate;
import java.util.*;

public final class S3IngestionWorkflow {
    private S3IngestionWorkflow() {}

    public static void runForever(DatalakeReader reader, DataMartService service, long pollMs) throws InterruptedException {
        Set<String> processedKeys = new HashSet<>();
        LocalDate currentDate = LocalDate.now();
        System.out.println("üì° Ingesta continua. D√≠a actual: " + currentDate);

        while (true) {
            try {
                LocalDate now = LocalDate.now();
                if (!now.equals(currentDate)) {
                    System.out.println("üîÑ Cambio de d√≠a (" + currentDate + " -> " + now + "). Reseteando estado...");
                    currentDate = now;
                    processedKeys.clear();
                }
                List<String> keys = reader.listFilesForDate(currentDate);
                List<String> newKeys = keys.stream().filter(k -> !processedKeys.contains(k)).toList();
                if (newKeys.isEmpty()) {
                    System.out.println("üò¥ No hay nuevos archivos. Esperando " + pollMs / 1000 + "s...");
                } else {
                    System.out.println("‚ûï Nuevos archivos: " + newKeys.size());
                    List<Map<String, Object>> routes = reader.readSpecificKeys(newKeys);
                    service.upsertToday(routes);
                    processedKeys.addAll(newKeys);
                    System.out.println("‚úî Marcadas como procesadas (" + processedKeys.size() + " total)");
                }
                Thread.sleep(pollMs);
            } catch (Exception e) {
                System.err.println("‚ùå Error en ciclo de ingesta: " + e.getMessage());
                Thread.sleep(Math.max(20_000, pollMs / 2));
            }
        }
    }
}
